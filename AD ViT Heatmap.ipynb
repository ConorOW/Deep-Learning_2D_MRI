{"cells":[{"cell_type":"markdown","source":["## A Vision Transformer for diagnosing AD - including a"],"metadata":{"id":"yI8dDaGF2Xex"}},{"cell_type":"code","execution_count":18,"metadata":{"id":"2sFjyG3jubFu","executionInfo":{"status":"ok","timestamp":1739853806145,"user_tz":480,"elapsed":1534,"user":{"displayName":"Conor Owens-Walton","userId":"10875353234421563263"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"86b19cc0-f77a-402f-d920-39b8690d6f3f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["### Check GPU availability"],"metadata":{"id":"G216LJK6SDtc"}},{"cell_type":"code","source":["import torch\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)"],"metadata":{"id":"Nj3FPiHl5eo2","executionInfo":{"status":"ok","timestamp":1739853912285,"user_tz":480,"elapsed":248,"user":{"displayName":"Conor Owens-Walton","userId":"10875353234421563263"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"18b572fc-8ad5-43f4-9650-59ae72572588"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]}]},{"cell_type":"markdown","source":["### Import libraries"],"metadata":{"id":"qZWn_jfdSaTp"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import transforms, datasets\n","import timm  # For Vision Transformer models"],"metadata":{"id":"FqJl_dKQSWUM","executionInfo":{"status":"ok","timestamp":1739853914533,"user_tz":480,"elapsed":249,"user":{"displayName":"Conor Owens-Walton","userId":"10875353234421563263"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["### Prepare the data"],"metadata":{"id":"26MJ6h3mTFne"}},{"cell_type":"code","source":["# Set directories and transform our images\n","# Data directories\n","train_dir = '/content/drive/MyDrive/AI_Projects/MRI/train'\n","test_dir = '/content/drive/MyDrive/AI_Projects/MRI/test'\n","\n","# Define transforms (resize images, convert to tensor, and normalize using ImageNet stats)\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                         std=[0.229, 0.224, 0.225])\n","])\n","\n","# Create datasets using ImageFolder (assumes subfolders correspond to class labels)\n","train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n","test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n","\n","# Data loaders\n","batch_size = 32\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n"],"metadata":{"id":"LEwacWdUScLi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create our vision transformer model"],"metadata":{"id":"eX1bPjYrUK7Z"}},{"cell_type":"code","source":["# Create a Vision Transformer model; for example, 'vit_base_patch16_224'\n","model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=2)\n","model = model.to(device)"],"metadata":{"id":"8ZvXq0HBT0j3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Define Loss Function and Optimizer"],"metadata":{"id":"5iJU0u4eUTNp"}},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)"],"metadata":{"id":"E8k9Z8ZhUVm7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Train our model"],"metadata":{"id":"SMKbOhW2UZ9R"}},{"cell_type":"code","source":["num_epochs = 3\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item() * images.size(0)\n","\n","    epoch_loss = running_loss / len(train_dataset)\n","    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"3gINgXd-UWr5","executionInfo":{"status":"error","timestamp":1739853428206,"user_tz":480,"elapsed":436919,"user":{"displayName":"Conor Owens-Walton","userId":"10875353234421563263"}},"outputId":"2df18a8c-b776-43dc-d81e-dcba4884842c"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-5a74fab730de>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["### Evaluate the model"],"metadata":{"id":"MSHf6RLXWLWm"}},{"cell_type":"code","source":["model.eval()\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        _, preds = torch.max(outputs, 1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","\n","accuracy = correct / total\n","print(\"Test Accuracy: {:.2f}%\".format(accuracy * 100))\n"],"metadata":{"id":"1jBrwAarWLIe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Heatmaps"],"metadata":{"id":"OXDwQzj0WNuq"}},{"cell_type":"code","source":["!pip install captum\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from captum.attr import Occlusion\n","\n","# Put the model in evaluation mode\n","model.eval()\n","\n","# Get one sample from the test set (e.g., the first image and its label)\n","images, labels = next(iter(test_loader))\n","# Select the first image and add a batch dimension: shape becomes (1, 3, 224, 224)\n","img = images[0].unsqueeze(0).to(device)\n","target_label = labels[0].item()\n","\n","# Create an Occlusion object for our model\n","occlusion = Occlusion(model)\n","\n","# Compute occlusion attributions. Here we occlude with a window of size 50x50 pixels.\n","# The input tensor shape is (1, 3, 224, 224), so we need sliding_window_shapes and strides as 4-tuples.\n","attributions = occlusion.attribute(\n","    img,\n","    strides=(1, 1, 8, 8),                   # don't stride on batch/channels, slide spatially\n","    sliding_window_shapes=(1, 3, 50, 50),     # occlude all channels over a 50x50 region\n","    target=target_label,\n","    baselines=0\n",")\n","\n","# Remove the batch dimension and convert to a NumPy array: shape (3, 224, 224)\n","attr_np = attributions.squeeze(0).detach().cpu().numpy()\n","\n","# To visualize as a heatmap, sum across channels to collapse to (224, 224)\n","heatmap = np.sum(attr_np, axis=0)\n","\n","# Plot the heatmap\n","plt.figure(figsize=(6, 6))\n","plt.imshow(heatmap, cmap='hot', interpolation='nearest')\n","plt.title('Occlusion Sensitivity Heatmap')\n","plt.colorbar()\n","plt.show()"],"metadata":{"id":"fYWfiUneWUkX"},"execution_count":null,"outputs":[]}],"metadata":{"jupytext":{"split_at_heading":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"colab":{"provenance":[{"file_id":"https://github.com/fastai/fastbook/blob/master/02_production.ipynb","timestamp":1698168436061}]}},"nbformat":4,"nbformat_minor":0}